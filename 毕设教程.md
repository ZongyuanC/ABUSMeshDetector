# 一种基于深度学习的腹壁疝检测方法

## 项目描述：

**2020本科毕业设计：自动化三维超声数据的计算机辅助分析**

------

使用卷积神经网络搭建的目标检测网络，对ABUS产生的三维超声腹壁疝补片数据进行补片目标检测

------

​		

**作者想对大家说的话：**

​		作为第一次从自己学习深度学习相关理论知识到自己动手实践自己的第一个神经网络开始，这已经是第十个月。在这十个月中，相信我和所有人一样，都是怀揣着一探神经网络究竟为何物的好奇心与探索精神一头扎进来，并且开始疯狂搜集资料，结果大都不算美好。繁琐的式子，大量的文字……这些都是阻挠我们继续深入这一领域的“当头棒喝”。当然，也有的人另辟蹊径，直接上手代码，做一个所谓的“调库大侠”，但我觉得这只能作为你对于初次探索神经网络的一次浅尝辄止的体验，不能算作你入门神经网络学习的基础。因为这种可以称之为“自以为是”的学习态度会使得你对神经网络的感觉更加朦胧，甚至不知其所言何物。

​		举个例子，某同学看到神经网络能够达到种种令人瞠目的“智能”效果，便与我大谈特谈，来回几次后，和我诉苦说自己并没有任何编写python代码的基础。我并不是要以此来嘲笑或者批判其他人，因为我本人也是一步一步摸着石头过河，蹚浑水蹚过来的。我想说的只是，进行应用研究时，我敢肯定我们所掌握的知识和技能是不能将整个项目囊括其中的，甚至从始至终都是你知识的盲区，但这才是学习和研究的常态不是吗？而所有使用的程序语言也仅只是我们将问题有逻辑有条理的进行建模后，用于呈现在计算机上进行演算的工具而已，语言的选择甚至不在学习神经网络的考虑范畴之内。我们所需考虑的是整个神经网络为什么能组成，为什么可以运行，其背后的数理逻辑和物理含义到底是什么，这才是我们进行学习研究的重点。所以，python也好，java也好，甚至是最近有人在游戏MC中制作出整个卷积神经网络的运行过程也罢，这都是我们学习的结果，是思考的具象化呈现。

​		层出不穷的神经网络背后，是繁复的数理逻辑堆叠连接。我知道去“啃”下整个神经网络的基础数学知识很枯燥，我承认我本人的数理逻辑不算很清晰，各种定理在离开书本例题后也不能灵活地应用到实际的数学建模中，但我还是咬牙去坚持看完，并尝试拿起纸笔手动推导，虽然其间我无数次睡着，挣扎着醒来，笔记本上已经画满了断断续续的笔触。我也很多次想过放弃，更换研究的方向，但又有什么研究是可以简单到没有任何困难，一路绿灯的呢？其实神经网络的数学基础都是你大学基础课程中所学过的，甚至能回溯到你的初中数学知识，比如一次函数。你可以权当做这次一次对你20年来的数理知识的一次大整合，相信你能将高等数学、线性代数等看似分离的学科融会贯通，找到它们各自的应用领域。

------

​		以上，说了这么多，借着咱学院由信息学院改为信息与人工智能学院的风头，我想将我这段时间的学习经验借由这个分享给大家，也算是你我同为校友的一份礼物。我会以我个人认为更加容易入门或者说更成体系的神经网络学习方法，来给大家讲解一个还算具有实用性的神经网络。毕竟理论堆砌后，直接进行神经网络的搭建与堆砌并不现实，我相信很多人也会一头雾水，因为知识的断层在每一次实践之前都会像雨后春笋一样突然涌现，打你一个措手不及。这时候，我相信有一个领路人的话，会让你有如神助，因为我个人觉得神经网络的入门是非常困难的，但深入其中后，又别有洞天。

------

​		为了让大家能够更有干劲，我会在下面将整个项目最终实现的效果先进行展示。

​		**源码地址：https://github.com/ZongyuanC/ABUSMeshDetector**

------



## 储备知识：



* **1、Python**  ->  廖雪峰Python ：https://www.liaoxuefeng.com/wiki/1016959663602400 

  ​		个人建议将函数def学完就可以上手进行编程了，因为python东西很杂，长时间不编写代码的话就容易忘记，而		且搭建神经网络的话，学到函数def就可以完全上手一个简单的神经网络搭建了。

* **2、线性代数**   ->  线性代数一定要学好，必须会矩阵运算！！！

* **3、概率论和信息论****   ->  现代的人工智能体系绝大都为概率模型的延伸应用，所以概率论和信息论是必不可少的！！

* **4、简单易入门书籍**   ->  《机器学习》周志华，清华大学出版社。

  ​		业内俗称“西瓜书”，因为封面全是西瓜，而且书中例举的都是以西瓜为代表的，如西瓜的表皮颜色，西瓜的表皮		纹路等等，用于指代实际应用中的种种问题。

  ![](G:\DOCX\GraduationProj\md教程\md图片\西瓜书.jpg)

* **5、全面且深入书籍**  ->  《深度学习》 Ian Goodfellow, Yoshua Bengio, Aaron Courville联合编著，人民邮电出版社。

  ​		业内俗称“花书”，因为封面全是花，内容与花没啥关系，硬要说的话就是Iris鸢尾花数据集。书中先铺垫了基本的		数理基础，如线性代数和概率论，然后才进入正式的机器学习理论知识讲解。

  ![](G:\DOCX\GraduationProj\md教程\md图片\花书.jpg)

* **6、慕课网络教程**  -> 《吴恩达机器学习》：https://mooc.study.163.com/smartSpec/detail/1001319001.htm

  ​		该课程是deeplearning.io和网易云课堂联合主办的深度学习课程，由吴恩达教授主讲，内容与花书相似，觉得啃		书困难的朋友可以自行订阅课程，课程是全免费的。

  ------

## 正文：

​		朋友们看到正文部分时，意味着整个项目终于要开始推进了。但我还是建议没有将上述流程走完的朋友耐下性子，因为应用研究是站在许多知识的基础建筑上，一点点往上堆砌的，这也是我苦口婆心说那么多的原因，若不是真的需要，我也不会那么铺张，毕竟本人也是很懒的。好了，现在正式开始。

## 一、模型选择

​		本文给大家带来的是一种基于深度学习的腹壁疝检测方法，也就是说，我们在任务之初必须要明确你的任务类型，是优化问题？回归问题？还是……？在这里，我们的问题很明显是目标检测(Object Detection)问题。

​		在明确了待解决问题的大方向后，开始寻找在此领域中适合的神经网络框架。我并不是不提倡大家从头开始搭建一个自己的专属网络，这很好。但在本文中，我将大家自己搭建并训练自己的神经网络默认为你在编程练习中已经尝试过，并且取得一定进展。

​		针对本文所需解决问题，我们总的来说是需要一个设计并制作一个能够在检测精度和运行速度上都具有较高性能的目标检测网络。所以，纵观直到2019年12月前，现今的所有目标检测算法中，能够满足要求的只有大名鼎鼎的You Only Look Once(YOLO)算法。本文选用的框架是YOLO-V3，其针对前两个版本已经做了许多优化，详细改动可以自行查看论文或是博客。YOLO-V3对我们来说，不仅仅是高性能的，更是易上手的，它能够实现近乎完美的高效目标检测，但没有使用任何冗余的运算单元。结构清晰明了，且代码完全开源，这是很多优秀的框架不可比拟的。你完全可以在将整个YOLO网络弄得熟门熟路后，在其基础上去设计或优化你自己的目标检测网络，这是事半功倍的，我会在后面详细讲解其各个部分的构成及作用，详细代码已经在第一部分项目描述中给出下载链接，各位可以自行下载使用。

## 二、数据处理

​		既然现在选用了开源的YOLO-V3框架，我们就先将重心放在需要“投喂”给神经网络，让神经网络能够进行模型“训练”的数据集上吧。

​		本文所需要的数据是自动化容积乳腺超声设备ABUS产生的三维超声数据，医学影像的一般格式为DICOM格式，除去我们所需要的图像数据信息后，还包含病人的相关隐私信息，所以要将其先剔除。

​		当然，如果你需要检测的目标已经有成熟的带标记的数据集，那么，你可以跳过数据处理章节。

------

### 1、冠状面抽取

​		由于本文中所使用的数据是三维超声数据，但YOLO-V3网络只是2-D层面的目标检测算法，并且对于初学者，且无专业医学影像专业背景，查阅除冠状面之外的矢状面或是横断面图像，都会倍感吃力，甚至找不到目标在超声图像中呈现出的形态，更不用提对其进行补片目标的标注了。为能更加清晰的给大家展现数据集制作的过程，我将其中一例离体实验数据展示给大家，给大家能有更加直观的认识。

​														![](G:\DOCX\GraduationProj\md教程\md图片\三平面.png)

​		图中所示的Coronal、Transverse和Sagittal分别为冠状面、横断面和矢状面。可以明显看出冠状面为最符合我们对补片的认知的，所以本文中的离体和载体实验数据都使用冠状面数据。

​		由于超声设备成像构造所限，原始超声图像直接呈现的图像数据均为不易于医生观察补片相关信息的横断面序列图像。为方便后续数据标定和数据集的制作，首先需要从ABUS三维原始数据提取出能够直观有效地呈现补片信息的二维冠状面数据。使用pydic库读取ABUS系统存储的DICOM格式的三维超声图像数据，根据DICOM数据帧头所携带的数据信息按照其像素代表的真实物理间隔将其冠状面还原并保存。表1展示的是抽取ABUS三维数据冠状面图像所需要的数据帧头属性信息。下表展示的是部分DICOM数据所携带的信息。

|       **属性**       |     **物理描述**      | **单位**  |
| :------------------: | :-------------------: | :-------: |
|     SizeX/Width      |   X方向的体数据尺寸   |   像素    |
| SizeZ/NumberOfFrames |   Z方向的体数据尺寸   |   像素    |
|    PhysicalDeltaX    | X方向的体像素物理间距 | 毫米/像素 |
|    PhysicalDeltaZ    | Z方向的体像素物理间距 | 毫米/像素 |

​		本文所使用的ABUS设备输出的DICOM文件中，所携带图像数据信息的部分以四维数组(SizeY,SizeX,1,SizeZ)的形式进行保存，其中1代表其体数据的每一个体素均以灰度信息保存，SizeY，SizeX，SizeZ分别为体数据在Y，X，Z方向上的尺寸，可视作在长度为SizeZ的Z方向上连续存储了分辨率为SizeY×SizeX的一系列二维灰度图像。由于图像还原时，无法展示其真实物理尺寸，所以根据下列式子进行冠状面真实物理尺寸的还原。
$$
X = SizeX × PhysicalDeltaX
$$

$$
Z = SizeZ × PhysicalDeltaZ
$$

抽取后获得的数据分布如下表所示。

|                | 离体 | 载体 |
| :------------: | :--: | :--: |
| 原始冠状面数据 | 125  |  75  |

​		由于本实验所用的ABUS三维超声腹壁疝补片数据较难获取，故原始数据量较少，容易造成目标检测网络模型的过拟合，使其最终在测试集上的检测效果较差。因此，要进行数据扩增。

------

### 2、数据扩增

​		数据扩增是指不增加实际的原始数据的情况下，对原始数据进行变换，从而创造出更多数据。

​		其目的在于增加原始数据的数量，丰富数据多样性，提高模型的泛化能力。

​		**图像扩增的基本方法：** 

   * 1 图像翻转

   * 2 图像旋转

   * 3 图像扭曲

   * 4 图像仿射变换等等。

     考虑其编码复杂性和实用性，本文采用图像翻转和图像旋转作为数据扩增的手段。

     以下为本文所使用的数据扩增过程示意图。

     ![](G:\DOCX\GraduationProj\md教程\md图片\数据扩增.png)

​		由上图可知，将原始图像分别顺时针旋转90度，并将其做水平镜像操作，得到7张新的冠状面图图像数据。进行数据扩增后的图像数据如下表所示。

|                    | 离体 | 载体 |
| :----------------: | :--: | :--: |
| 扩增后的冠状面数据 | 1000 | 600  |

------

### 3、数据标注

​		前面的冠状面抽取和数据扩增都是为了进行数据标注，那么数据标注到底是什么？

​		相信很多人一开始接触神经网络时，都知道要用数据去训练神经网络，可是“投喂”给神经网络的数据到底是什么样子，才能让神经网络能够进行训练呢？当时这个问题困扰我还蛮久的。现在大家可以跟我一起一探究竟。

​		标注图像并不单单是在图像上将待检测目标框起来这么简单，还需要根据每一幅原始图像数据标记出每幅图像所含目标的类别信息，位置信息和大小信息，甚至是姿态信息和角度信息，这些信息才是最终送入网络的数据。

​		本文使用的是LabelImage插件，标注后的冠状面图像和数据展示如下，数据会以.xml文件的形式保存。

![](G:\DOCX\GraduationProj\md教程\md图片\标注.png)

​		将1600张原始图像数据全都标记完毕后，将所有数据作为实验的正样本数据，并将其按照9:1的比例拆分为训练集和测试集。

------

### 4、数据集制作

​		数据集建议制作成VOC数据集的形式，VOC数据集为PASCAL VOC挑战赛的专用数据集。

​		编写Python脚本，将上述步骤中得到的补片标定数据制作成VOC数据集格式，得到的数据集文件结构如图5所示。其中Annotations中.xml形式存放待检测对象的标注信息，即上述得到的1600个标注信息，且必须与JPEGImages中的图像名称一致；JPEGImages中包括4个子文件夹，Action中存放图像中人物的动作信息，Layout存放图像中的人体部位(如头、手等)，Segmentation中存放用于目标分割的数据，Main中存放对应的数据及图片名称，并将其按类别分类，如训练集train.txt；JPEGImages中存放所有与标注信息名称相同的.jpg图像文件；SegmentationClass存放用于语义分割的图片文件；SegmentationObject存放用于实例分割的图片文件。目录结构如下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\VOC.png)

------

## 三、训练网络

### 1、环境搭建

* **Python  ->  3.7.1**

* **Tensorflow-gpu   ->   1.13.1** 【有条件的话必须使用GPU加速训练，否则本文所述训练耗时均为几十小时，才可进行下一次调参，非常痛苦】若非要安装非gpu版本的gpu，可以安装tensorflow  ->  1.14.0

  （下载时建议使用国内豆瓣镜像源，否则非常非常非常慢！）

* **keras**   ->   2.1.4 

* **Cuda**   ->   10.0.0 【若不使用GPU，则不需要安装此环境】

* **Cudnn**   ->   7.3.1  【若不使用GPU，则不需要安装此环境】

注意：以上环境版本必须互相契合，不是一味追求最新的就是最好！不然会出现很多兼容性问题！

------

### 2、YOLO-V3架构解析

​		由于YOLO-V3框架并不是直接对目标区域进行预测，而是在训练过程中先计算出所有的预测框后，再进行最终预测框的选定。

​		所以整个YOLO-V3框架可以分为两个大部分，分别为训练和预测两个部分。整个YOLOV3框架示意图我绘制在下图中进行展示。

![](G:\DOCX\GraduationProj\md教程\md图片\YOLOV3框架.png)

#### 2.1 训练【卷积神经网络部分】

##### 2.1.1 前置网络 Darknet53

​		使用Darknet53作为YOLOV3的前置网络，也就是特征提取网络。原因是YOLOV3并不意味追求高速度，而是要在保持较高的检测速度时，确保具有较高精度。所以选择了略为牺牲精度而大幅提高精度的Darknet53网络。大量使用残差网络中的残差单元Res跳层shortcut进行连接，使得原始的图像输入信息能够直接传输到更深层的网络中进行后续处理，因为当输入数据在浅层网络中的训练准确率达到饱和后，进入到更深层次的网络继续训练会导致训练准确率不升反降。因此引入shortcut能够使训练模型的精度和训练速度得到质的飞跃。前置网络中通过改变卷积核的大小size与步长stride来控制张量的尺寸，替代一般使用的最大池化层MaxPooling，本实验中使用的大都为步长stride为2的1×1和3×3的卷积核进行降采样。使用卷积操作替代池化操作可以有效降低池化操作带来的梯度负面效果，如梯度消失或者梯度爆炸，这将会使得后续的深层网络训练失效。

![](G:\DOCX\GraduationProj\md教程\md图片\backbone.png)

​		上图为本文中所使用的Darknet53网络结构示意图，注意，此处的Darknet53并不是完整的，它只包含52个卷积层Conv，舍弃了最终的全连接层FC，因为全连接层FC最终要与激活函数Softmax相接进行分类输出，但YOLOV3中并不是直接输出各个预测框的类别，所以并不需要。

------

##### 2.1.2 多尺度特征检测

​		由于网络的加深，输入图像由416×416经过降采样后，损失了过多的空间信息。目标检测可以简单地理解为分类问题+定位问题。深度较深的网络适用于设计分类网络，因为在空间信息损失的过程中，特征图所含的类别信息会大大增加。但定位问题就恰恰相反。所以采用借鉴FPN算法设计多尺度特征检测模型，将具有较多空间信息的特征图和具有较多分类信息的特征图进行张量拼接。在保证目标检测精度的同时，也能够使得模型对大小不同的检测目标进行检测，提高模型的泛化性。

​		**A、 13×13×18的特征图Feature Map**

​		经过前置网络后，输出13×13×1024的特征图，然后经过7次卷积Conv操作后，在网络上第82层输出13×13×18的特征图。此特征图是粗网格，感受野较大，用于检测较大的目标。

​		**B、 26×26×18的特征图Feature Map**

​		将网络第79层的特征图复制后，经过2倍上采样，与网络第61层的输出进行张量拼接Concat，然后经过7次卷积操作，在网络上第94层得到26×26×18的特征图。此特征图用于检测一般大小的目标。

​		**C、 52×52×18的特征图Feature Map**

​		与上述过程相同，将网络第97和36层的特征图进行张量拼接后，可在第106层得到52×52×18的特征图，此特征图感受野较小，用于检测较小目标。

​		上述张量拼接过程图下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\特征融合.png)

​		三个尺度的网格如下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\多尺度.png)

------

##### 2.1.3 损失函数

​		由于YOLO系列的论文中，只有YOLO第一个版本中给出了具体的损失函数表达式。在此，我结合源码与YOLOV3原文将YOLOV3框架所使用的损失函数Loss进行复现，其结果如下。

![](G:\DOCX\GraduationProj\md教程\md图片\损失函数计算公式.png)

​		由上式可明显看出，Loss由3个部分线性加和组成，分别为依次为置信度损失Lconf，类别损失Lcla和位置损失Lloc。

​		其实现在已有许多损失函数优化器Optimizer封装在keras中，只要合理调用，并且损失函数loss合理，都可以让YOLOV3模型顺利进行魔性训练。因此不再赘述其各个部分如何计算，详细计算公式放在我的本科毕业论文中，需要自取。

------

##### 2.1.4 网络输出结果

​		以13×13×18的特征图为例进行讲解。其结构展示如下，其中13×13代表网格数量，其中每个网格中有18个信息量。由于每个网格可以预测3个物体，所以每6个信息量为一个预测框所含信息(tw,th,tx,ty,confidence,class)，4个位置信息，1个置信度信息，1个类别信息，此处只有1类，因为数据集中只有1类待检测的补片目标。若为COCO数据集的80类，那么输出的feature map通道数应为3×(4+1+80)=255。

![](G:\DOCX\GraduationProj\md教程\md图片\特征图展示.png)

​		每一幅图像送入到网络后，最终会在3个YOLO输出层产生10647个预测框【3×（13×13+26×26+52×52）】，

为了提高网络的运算速度，设置一个置信度阈值Score_th，将置信度过低的预测框先滤除。

​		然后计算目标性评分Object，即计算特征图中置信度与类别的联合概率。

​		这一操作会将特征图的3×3×15的形式，即(tw,th,tx,ty,object)，作为最终的训练部分输出。

------

#### 2.2 预测【NMS】

​		由于训练部分输出的最终特征图可能会在对同一个检测目标上出现多个预测框，如下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\NMS效果展示.png)

​	因此要使用非极大值抑制算法NMS对其进行预测框滤除，其具体步骤为：

* 1、将经过上述操作得到的所有特征图按照object进行降序排列。

* 2、取出现在排位最前的预测框，并计算该预测框与其他预测框的交并比IoU，IoU大于预先设置的IoU_th的预测框就将其滤出。这里IoU大于阈值的表示同一个物体被多个预测框选中了，此时只保留目标性评分最高的即可；低于阈值的可视作另一个物体。

* 3、重复上述步骤1和2，直到滤除了所有的冗余预测框。

  上述过程如下图所示。
  
  ![](G:\DOCX\GraduationProj\md教程\md图片\NMS流程图.png)
  
  **经过上述步骤可以得到最终的补片预测框。到此为止，可以说整个检测过程已经完成了绝大部分，后续只需将最终得到的预测框信息进行坐标转换并显示在图像上即可。**

------

#### 2.3 坐标转换

​		由于YOLOV3框架在设计时，训练得到的(tw,th,tx,ty)四个位置参数是预测框最终的相对位置，所以还要将其转换为绝对位置后才能进行有效地补片目标检测。公式如下。

![](G:\DOCX\GraduationProj\md教程\md图片\坐标转换公式.png)

下图为预置锚框Anchor与预测框实际位置的转换关系示意图。

![](G:\DOCX\GraduationProj\md教程\md图片\坐标转换示意图.png)

​		上述预置锚框Anchor是根据COCO数据集经过K-Means聚类的到的候选框，共9个，分为三组，对应不同的特征图感受野。其对应关系如下图所示。

​		![](G:\DOCX\GraduationProj\md教程\md图片\锚框对应关系.png)

​		表中的锚框尺寸仅代表其大小，没有实际的位置信息。

​		转换关系式中的sigmoid函数的作用是将预测框相对于当前网格中心点的偏移量缩放到0和1之间，表示预测框相对于网格中心点的偏移量，这样能够将预测框与当前网格绑定。Cx和Cy分别为当前网格的中心点坐标。由公式转换得到的(bx,by,bw,bh)为最终的补片目标位置。

------

## 四、实验及结果

### 3.1 实验设备

* **1、处理器   ->   Intel Xeon E5 2678 V3 2.50GHz**

* **2、处显卡   ->   单卡NVIDIA GeForce GTX 2080Ti**

* **3、操作系统   ->   Windows10**

  ------

### 3.2 超参数设置

* **1、Epoch   ->   100**

* **2、Batch Size   ->   10**

* **3、 Weight Init   ->   COCO数据集训练所得权重【使用此权重并无特殊含义，只要使用非全0的初始权重即可，例如高斯分布也行。】**

* **4、Optimizer   ->   Adam**

* **5、 Learning Rate   ->   随着迭代次数的增加不断变化，开始很小，然后逐渐递增，再减小。**

  ------

### 3.3 实验结果

​		选择待检测的测试图像后，送入上述搭建好的网络中进行补片目标检测，可输出待检测图像中有无补片目标，目标性评分及预测框形状参数，以其中一例 离体 实验图像数据检测结果为例，如下图所示。

​											![](G:\DOCX\GraduationProj\md教程\md图片\离体检测结果.png)

​										![](G:\DOCX\GraduationProj\md教程\md图片\检测数据.png)

下面再展示一下载体实验检测结果。

![](G:\DOCX\GraduationProj\md教程\md图片\载体检测结果.png)

------

### 3.4 网络性能评估

​		将测试集数据的补片检测结果与其正确的标注Ground Truth进行比较后，可以计算出本文所述目标检测网络对ABUS补片数据进行补片目标检测的准确率如下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\mAp.png)

​		由于本文中只进行了一类补片的目标检测，所以其类别平均精度和类别精度相等。

​		为了能够实际应用到CAD系统中，网络检测效果能够达到实时识别补片目标的效果，除去补片识别的精度外，还需要考虑其识别速度。因此进行对腹壁疝补片实时检测的帧率测试，分别在实验环境所述的CPU和GPU硬件条件下进行实验，其结果下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\帧率.png)

​		

​		以其中一例ABUS产生的DICOM格式的测试超声图像数据为例进行补片目标检测测试。某三维超声数据形式为(Y,X,Z)=(565,719,318)，先抽取出565帧(719,318)冠状面图像数据，该过程耗时0.142秒，将其送入上述本设计中已经训练好的目标检测网络中进行补片目标检测。检测所需时间由硬件环境所决定，真实数据如下图所示。

![](G:\DOCX\GraduationProj\md教程\md图片\耗时检测.png)

​		由上面两幅图所展示的数据可以看出，在GPU硬件加速条件下，本文所设计的卷积神经网络可以在95.44%的高精确率条件下达到27帧/秒的检测帧率，整个检测过程可在20.64秒完成，基本实现了对ABUS腹壁疝补片目标的实时检测需求。

------

## 五、结语

​		综上所述，本文所提出的基于卷积神经网络的腹壁疝补片检测方法对现今医师进行三维超声图像数据的阅片工作有较为出色的筛查检测作用，能够大大降低医师阅片的工作量，将成百张帧的超声图像缩减至几十帧甚至几帧，并以更为人性化的冠状面图像进行图像数据显示；同时以高精度预测出待检测的补片目标位置，有效防止在阅片时出现对微弱异常区域的漏诊和误诊，辅助医师高效快速地锁定补片位置，并根据补片位置形态对后续的疝区进行评估诊断，为今后的治疗提供了强有力的基础，因此本方法具有重要的临床指导意义。

​		本文写完的时候，YOLOV4已经全面问世，但并未在YOLOV3的基础上做出很大的理论创新，更多的是替换相应部分的网络结构，使得其训练消耗的算力更小，速度更快，精度更高。大家可以在学习YOLOV3的基础上自己进行探索与改进。